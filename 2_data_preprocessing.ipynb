{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e7f01c",
   "metadata": {},
   "source": [
    "#### Data pre-processing\n",
    "- run script to generate csv files that can be uploaded to MySQL via an sql script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb75c77",
   "metadata": {},
   "source": [
    "#### 0. unpacking data files\n",
    "- script to preview dataset to choose relevant columns to save in unpacking <br>\n",
    "- script to unpack full dataset but saving only the relevant columns <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e0cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e002ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview first n lines\n",
    "# files to unpack: Books_5.json.gz, meta_books.json.gz, goodreads_reviews_dedup.json.gz, \n",
    "\n",
    "data, n = [], 100 # n = number of lines to extract\n",
    "with gzip.open('goodreads_reviews_dedup.json.gz') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= n:\n",
    "            break  # Stop after reading the first 100 lines\n",
    "        else:\n",
    "            # Parse the line as a JSON object\n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "            # Extract only the first 100 key-value pairs (can change)\n",
    "            limited_json_object = dict(list(json_object.items())[:100])\n",
    "            data.append(limited_json_object)\n",
    "            \n",
    "# convert list to df\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "# preview df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c09bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list columns; if need to copy-paste\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to unpack json.gz to csv\n",
    "\n",
    "def extract_to_csv(json_gz_file, csv_filename, cols_to_extract):\n",
    "    '''\n",
    "    Extracts data from a gzipped JSON file and saves the data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    json_gz_file (str): The path to the input gzipped JSON file.\n",
    "    csv_filename (str): The desired name of the output CSV file.\n",
    "    cols_to_extract (list): A list of column names to be extracted from the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    None. Saves the data to a CSV file.\n",
    "\n",
    "    '''\n",
    "    # Initialize an empty list to store the data\n",
    "    data_list = []\n",
    "\n",
    "    # Initialize a counter for progress tracking\n",
    "    line_counter = 0\n",
    "\n",
    "    # Record the start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Read the gzip file\n",
    "    with gzip.open(json_gz_file, 'r') as f:\n",
    "        # Read line by line and extract desired columns\n",
    "        for line in f:\n",
    "            line_counter += 1\n",
    "            if line_counter % 1000000 == 0:\n",
    "                current_time = time.time()\n",
    "                elapsed_time = int(current_time - start_time)\n",
    "                print(f\"Processed {line_counter} lines in {elapsed_time} seconds.\")\n",
    "            data = json.loads(line.decode('utf-8'))\n",
    "            extracted_data = {column: data.get(column) for column in cols_to_extract}\n",
    "            data_list.append(extracted_data)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data_list)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40055a59",
   "metadata": {},
   "source": [
    "#### 1 Pre-processing data\n",
    "- Amazon dataset: preprocessing to supplement author information, generating individual relations tables in ER\n",
    "- goodreads: preprocessing to the form RDMS schema require\n",
    "- blackwells (AI-generated reviews): supplementing with asin, as well as randomly generated ratings and n-votes. Code to artificially generate reviews are in a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67259e",
   "metadata": {},
   "source": [
    "##### 1.1 Preprocessing Amazon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4169a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'user', 'review', 'user_review', 'item_review' tables\n",
    "\n",
    "amazon5coredf = pd.read_csv('Books_5.csv')\n",
    "\n",
    "# 'user' relation\n",
    "userdf= amazon5coredf[['reviewerID','reviewerName','verified']]\n",
    "user_df_duplicates_removed = userdf.drop_duplicates(subset=['reviewerID'], keep='first')\n",
    "user_df_duplicates_removed.to_csv('user.csv', index=False)\n",
    "\n",
    "# 'review' relation\n",
    "reviewdf= amazon5coredf.reset_index().rename(columns={'index': 'rid'})\n",
    "reviewdf['rid']+=1\n",
    "review_df= reviewdf[['rid','overall','reviewTime','summary','unixReviewTime','vote']]\n",
    "review_df.to_csv('review.csv', index=False) \n",
    "\n",
    "# 'user_review' relation\n",
    "user_reviewdf = reviewdf[['rid','reviewerID']]\n",
    "user_reviewdf.to_csv('user_review.csv', index=False) \n",
    "\n",
    "# item_review relation\n",
    "item_reviewdf = reviewdf[['rid','asin']].rename(columns={'asin': 'book_asin'})\n",
    "item_reviewdf.to_csv('item_review.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For AMAZON FULL BOOK INFO\n",
    "\n",
    "amazonmeta = []\n",
    "with gzip.open('meta_books.json.gz') as f:\n",
    "    for i, line in enumerate(f):\n",
    "#         if i >= 10000:\n",
    "#             break  # Stop after reading the first 1000000 lines\n",
    "#         else:\n",
    "#             # Parse the line as a JSON object\n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "            # Extract only the first 6 key-value pairs\n",
    "            limited_json_object = dict(list(json_object.items())[:100])\n",
    "            amazonmeta.append(limited_json_object)\n",
    "\n",
    "df= pd.DataFrame.from_dict(amazonmeta)\n",
    "df3 = df.fillna('')\n",
    "df4 = df3[df3.title.str.contains('getTime')] # unformatted rows\n",
    "amazonmetadf= df3[~df3.title.str.contains('getTime')] # filter those unformatted rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda69842",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For AUTHOR INFO TO CREATE AUTHOR TABLE\n",
    "\n",
    "authordata = []\n",
    "with gzip.open('goodreads_book_authors.json.gz') as f:\n",
    "    for i, line in enumerate(f):\n",
    "#         if i >= 10:\n",
    "#             break  # Stop after reading the first 100 lines\n",
    "#         else:\n",
    "#             # Parse the line as a JSON object\n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "            # Extract only the first 6 key-value pairs\n",
    "            limited_json_object = dict(list(json_object.items())[:100])\n",
    "            authordata.append(limited_json_object)\n",
    "\n",
    "authordf= pd.DataFrame.from_dict(authordata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b76f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For AUTHOR INFO TO AUGMENT AMAZON DATASET\n",
    "\n",
    "kagglebookdf = pd.read_csv('books_data.csv')\n",
    "\n",
    "##Convert authors column from string into array\n",
    "\n",
    "def safe_literal_eval(value):\n",
    "    if not pd.isna(value):\n",
    "        return ast.literal_eval(value)\n",
    "    return value  # Keep NaN values as they are\n",
    "\n",
    "kagglebookdf['authors'] = kagglebookdf['authors'].apply(safe_literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##For BOOK INFO TO AUGMENT AMAZON FULL BOOK INFO\n",
    "\n",
    "goodreads_bookdf = pd.read_csv('goodreads_books.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplement Authors in main dataset\n",
    "\n",
    "####Replace empty cells with NaN\n",
    "\n",
    "def replace_empty_array(cell):\n",
    "    if isinstance(cell, list) and not cell:\n",
    "        return np.nan\n",
    "    return cell\n",
    "\n",
    "###Convert cells to array \n",
    "\n",
    "def convert_to_array(cell):\n",
    "    if cell is not np.nan:\n",
    "        return [cell]\n",
    "    return np.nan\n",
    "\n",
    "amazonmetadf = amazonmetadf.applymap(replace_empty_array)\n",
    "\n",
    "overalldf = amazonmetadf.replace('', np.nan)\n",
    "\n",
    "\n",
    "###Join Author table in Goodreads onto Amazon Table and convert author list to array\n",
    "\n",
    "merged_df = pd.merge(overalldf, authordf, left_on='brand', right_on='name', how='left')\n",
    "merged_df['author'] = merged_df['name']\n",
    "merged_df = merged_df.drop(['brand','name','author_id','average_rating','text_reviews_count','ratings_count'], axis=1)\n",
    "merged_df['author'] = merged_df['author'].apply(convert_to_array)\n",
    "\n",
    "\n",
    "###Join Author table in Kaggle onto Amazon table to create a complete set of authors (where possible) in Amazon set\n",
    "\n",
    "author_dict = kagglebookdf.set_index('Title')['authors'].to_dict()\n",
    "\n",
    "def update_author(row):\n",
    "    title = row['title']\n",
    "    authors = author_dict.get(title)\n",
    "    \n",
    "    if authors is not None:\n",
    "        if row['author'] is np.nan:\n",
    "            row['author'] = authors if isinstance(authors, list) else [authors]\n",
    "        else:\n",
    "            if not isinstance(authors, float):\n",
    "                if not isinstance(row['author'], list):\n",
    "                    row['author'] = [row['author']]\n",
    "                for author in authors:\n",
    "                    if author not in row['author']:\n",
    "                        row['author'].append(author)\n",
    "    return row\n",
    "\n",
    "merged_df = merged_df.apply(update_author, axis=1)\n",
    "\n",
    "merged_df= pd.merge(merged_df, goodreads_bookdf, left_on='asin', right_on='isbn', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean main dataset (price, rank)\n",
    "\n",
    "# Data Cleaning of Main Merged Df for Items Relation\n",
    "\n",
    "#Convert price to float for price column\n",
    "def clean_and_convert_price(price_str):\n",
    "    try:\n",
    "        if isinstance(price_str, str):\n",
    "            cleaned_price = price_str.replace('$', '').replace(',', '')\n",
    "            return float(cleaned_price)\n",
    "        else:\n",
    "            return price_str\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "merged_df['price'] = merged_df['price'].apply(clean_and_convert_price)\n",
    "\n",
    "# Convert rank to int for rank column\n",
    "def extract_integer(text):\n",
    "    try:\n",
    "        match = re.search(r'(\\d[\\d,]*)', text)\n",
    "        if match:\n",
    "            return int(match.group().replace(',', ''))\n",
    "        else:\n",
    "            return np.nan\n",
    "    except (TypeError, ValueError,AttributeError):\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to the 'Rank' column\n",
    "merged_df['rank'] = merged_df['rank'].apply(extract_integer)\n",
    "\n",
    "# # Display the modified DataFrame\n",
    "# merged_df= merged_df.rename(columns={'description_x': 'description','title_x':'title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break up tables for export\n",
    "\n",
    "# 'item' relation\n",
    "itemdf = merged_df[['asin','title','description','price','rank','publisher','publication_year']]\n",
    "itemdf.to_csv('items.csv', index=False)\n",
    "\n",
    "# 'item_author' relation\n",
    "author_expanded_df = merged_df.explode('author')\n",
    "author_expanded_df = pd.merge(author_expanded_df, authordf, left_on='author', right_on='name', how='left')\n",
    "item_author = author_expanded_df[['asin','author_id']].dropna()\n",
    "item_author.reset_index().drop(columns=['index'])\n",
    "item_author.to_csv('item_author.csv', index=False)\n",
    "\n",
    "# 'item_category' and 'category' relations\n",
    "\n",
    "cat_expanded_df = merged_df.explode('category')\n",
    "item_cat = cat_expanded_df[['asin','category']].dropna()\n",
    "\n",
    "item_cat2 = item_cat.reset_index().drop(columns=['index'])\n",
    "item_cat3 = item_cat2[~(item_cat2['category']=='Books')]\n",
    "\n",
    "unique_categories = item_cat3['category'].unique()\n",
    "category_dict = {index+1: category for index, category in enumerate(unique_categories)}\n",
    "categorydf= pd.Series(category_dict).to_frame().reset_index()\n",
    "categorydf = categorydf.rename(columns={'index': 'cid', 0:'name'})\n",
    "\n",
    "item_categorydf = pd.merge(item_cat3, categorydf, left_on='category', right_on='name', how='left').drop(columns=['category','name'])\n",
    "\n",
    "categorydf.to_csv('category.csv', index=False) \n",
    "item_categorydf.to_csv('item_category.csv', index=False) \n",
    "\n",
    "# 'also_bought' and 'also_viewed' relations \n",
    "\n",
    "bought_expanded_df = merged_df.explode('also_buy')\n",
    "also_bought = bought_expanded_df[['asin','also_buy']].dropna()\n",
    "\n",
    "viewed_expanded_df = merged_df.explode('also_view')\n",
    "also_viewed = viewed_expanded_df[['asin','also_view']].dropna()\n",
    "\n",
    "#Export\n",
    "also_viewed.to_csv('also_viewed.csv', index=False) \n",
    "also_bought.to_csv('also_bought.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38c9a3",
   "metadata": {},
   "source": [
    "##### 1.2 Preprocessing Goodreads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_goodreads_reviews = pd.read_csv('goodreads_reviews.csv')\n",
    "df_goodreads_books = pd.read_csv('goodreads_books.csv')\n",
    "df_asin_amazon = pd.read_csv('asin_amazon.csv')\n",
    "\n",
    "# extract required columns\n",
    "df_goodreads_reviews = df_goodreads_reviews[['book_id', 'rating', 'review_text', 'n_votes']] \n",
    "df_goodreads_books = df_goodreads_books[['isbn', 'book_id']]\n",
    "\n",
    "# incorporate 'isbn' to reviews table\n",
    "df_external = pd.merge(df_goodreads_reviews, df_goodreads_books, \n",
    "                       left_on = 'book_id', right_on = 'book_id', how = 'left')\n",
    "\n",
    "# remove NaN values, rename columns to be consistent with ER\n",
    "df_external_dropped = df_external.dropna()\n",
    "df_external_dropped = df_external_dropped.rename(columns = {'isbn': 'asin'})\n",
    "df_external_dropped['platform'] = 'goodreads'\n",
    "df_external_dropped = df_external_dropped[['rating', 'review_text', 'n_votes', 'platform', 'asin']]\n",
    "# select a random subset of data to lower data size; \n",
    "    # only using this as a proof-of-concept demonstrate feasibility of merge an external dataset to main (Amazon) dataset\n",
    "df_external_select = df_external_dropped.sample(frac=0.37, random_state = 73)\n",
    "df_external_select"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53abe3",
   "metadata": {},
   "source": [
    "#####  1.3 Preprocessing Artificially generated reviews (aka 'blackwells'), and merging with goodreads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b80069",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('10k_gen_text_review.csv')\n",
    "df = df.rename(columns={'Title': 'title'})\n",
    "\n",
    "# load metabooks data to join on 'title' to get 'asin'\n",
    "df_books = pd.read_csv('meta_books.csv')\n",
    "df_title_asin = df_books[['title', 'asin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a wrapper function to perform 'v-lookup'\n",
    "def filterWrapper(fulldataframe, HeaderName=\"\", lst=[\"\"]):\n",
    "    df_copy = fulldataframe.copy()\n",
    "    if type(lst[0]) is str:\n",
    "        lst_lowercase = [i.lower() for i in lst]\n",
    "        df_out = df_copy[df_copy[HeaderName].str.lower().isin(lst_lowercase)]\n",
    "    elif type(lst[0]) is int:\n",
    "        df_out = df_copy[df_copy[HeaderName].isin(lst)]\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper to perform 'v-lookup'\n",
    "df_out = filterWrapper(df_title_asin, 'title', df['title'].to_list())\n",
    "df_out['title'] = df_out['title'].str.lower() # df_out has columns 'title' and 'asin'\n",
    "\n",
    "# incorporate 'asin' to df from blackwells\n",
    "merged_df = df.merge(df_out, on='title', how='left')\n",
    "merged_df_dropped = merged_df.dropna()\n",
    "\n",
    "# rename columns\n",
    "df_gen = merged_df.rename(columns={'GeneratedReview': 'review_text'})\n",
    "df_gen = df_gen[['review_text', 'asin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorporate randomly generated ratings and rating votes\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Add the 'rating' column with random integer values from 1 to 5\n",
    "df_gen['rating'] = np.random.randint(1, 6, size=len(df_gen))\n",
    "\n",
    "# Add the 'n_votes' column with Poisson distributed integer values from 0 to 100, lambda=37\n",
    "df_gen['n_votes'] = np.random.poisson(lam=37, size=len(df_gen))\n",
    "\n",
    "# Add the 'platform' column with the value 'blackwells'\n",
    "df_gen['platform'] = 'blackwells'\n",
    "\n",
    "# save to csv\n",
    "df_gen.to_csv('blackwells.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging with goodreads\n",
    "df_blackwells = pd.read_csv('blackwells.csv')\n",
    "df_external_combined = pd.concat([df_external_select, df_blackwells], axis=0).reset_index(drop=True)\n",
    "\n",
    "# add primary key 'eid'\n",
    "df_external_combined = df_external_combined.reset_index().rename(columns={'index':'eid'})\n",
    "df_external_combined['eid'] += 1\n",
    "\n",
    "# split external into 'item_external' and 'external' relations\n",
    "df_external = df_external_combined[['eid', 'review_text', 'rating', 'n_votes', 'platform']]\n",
    "df_item_external = df_external_combined[['eid', 'asin']]\n",
    "\n",
    "# save to csv\n",
    "df_external.to_csv('external.csv', index = False)\n",
    "df_item_external.to_csv('item_external.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
